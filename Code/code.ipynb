{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b><i>Video Game Sales Prediction Analysis</i></b>\n",
    "**Created By :** Sai Kiran Sajja (Roll No : 21CSB0A49) <br>\n",
    "**Gmail :** sk21csb0a49@student.nitw.ac.in <br>\n",
    "**Date:** November 2, 2023\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <b>Introduction</b>\n",
    "\n",
    "The `Video Game Sales Prediction` project is a comprehensive analysis aimed at understanding and predicting the sales of video games in the dynamic and ever-evolving `gaming industry`. With the rapid growth of this sector, it has become increasingly important for `game developers`, `publishers`, and `stakeholders` to gain insights into the factors influencing a game's commercial success.\n",
    "\n",
    "This project leverages **`Artificial Intelligence`** and **`Data Analysis`** techniques to predict video game sales based on various game attributes, historical data, and market trends. By utilizing **`Machine Learning`**, we aim to provide a valuable tool for industry professionals and enthusiasts to make data-driven decisions, whether it's for `game development`, `marketing strategies`, or `investment opportunities`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Objectives**\n",
    "\n",
    "1. **Data Exploration and Preprocessing:** The project begins with a detailed exploration of the `dataset`, including `data cleaning`, handling `missing values`, and feature `engineering` to prepare the data for `analysis`.\n",
    "\n",
    "2. **Genre and Sales Analysis:** We investigate the distribution of `video game genres` and analyze the relationship between `genres` and `global sales`. This `exploration` sheds light on popular game categories and their commercial success.\n",
    "\n",
    "3. **Correlation Analysis:** We examine the `correlations` between various game attributes to understand which `features` play a significant role in determining `sales`. This `analysis` helps in identifying the factors that strongly impact `sales figures`.\n",
    "\n",
    "4. **Model Building and Prediction:** Using **`Machine Learning`**, we build a `predictive model` to forecast `video game sales`. We employ the **`XGBoost algorithm`** to develop a model that can make accurate predictions based on the available data.\n",
    "\n",
    "5. **Model Evaluation:** The project assesses the model's performance by using metrics such as `R-squared`, `Root Mean Squared Error (RMSE)`, and `Mean Absolute Error (MAE)`. These evaluations provide insights into the `accuracy` and `reliability` of the sales predictions.\n",
    "\n",
    "6. **Additional Visualizations:** Beyond the `predictive model`, we create various `visualizations` to present the `data` in an easily digestible format. These `visualizations` offer a comprehensive understanding of the `dataset` and the model's performance.\n",
    "\n",
    "The `Video Game Sales` Prediction project serves as a valuable resource for those interested in the `gaming industry`, from `game developers` seeking to understand market trends to `investors` making informed decisions. By harnessing the power of **`Artificial Intelligence`** and **`Data Analysis`**, we aim to provide actionable insights and facilitate more informed `decision-making` in the world of `video game sales`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Video Game Sales Dataset**\n",
    "\n",
    "The `dataset` used in this project was sourced from **`Kaggle`** and originated from a web scrape of `vgchartz.com`. It provides comprehensive information about `video games`, including their `release` details and `sales figures`. Below are the fields included in the dataset:\n",
    "\n",
    "- **Name**: The title of the video game.\n",
    "- **Platform**: The platform on which the game was released (e.g., PC, PS4, etc.).\n",
    "- **Year_of_Release**: The year in which the game was officially released.\n",
    "- **Genre**: The genre or category to which the game belongs.\n",
    "- **Publisher**: The company responsible for publishing the game.\n",
    "- **NA_Sales**: Sales in North America (in millions).\n",
    "- **EU_Sales**: Sales in Europe (in millions).\n",
    "- **JP_Sales**: Sales in Japan (in millions).\n",
    "- **Other_Sales**: Sales in the rest of the world (in millions).\n",
    "- **Global_Sales**: The total worldwide sales of the game.\n",
    "- **Developer**: The entity that developed the game.\n",
    "- **Rating**: The rating assigned to the game.\n",
    "\n",
    "This `dataset` serves as the foundation for the `Video Game Sales Prediction` project, allowing us to explore and analyze the factors contributing to the `commercial success` of video games. With this information, we can develop `predictive models` and generate insights for the `gaming industry`.\n",
    "\n",
    "[Link to the Dataset on Kaggle](https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Code</b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Data Preprocessing</b>\n",
    "### **Importing the necessary Libraries**\n",
    "We start by importing the necessary libraries, including `pandas`, `numpy`, `matplotlib`, `seaborn`.<br><br>\n",
    "There are more imported libraries like `Scikit-Learn`(`SimpleImputer`, `ColumnTransformer`, `OneHotEncoder`, `XGBRegressor`, `r2_score`, `mean_squared_error`, `mean_absolute_error`)  later in the code for data cleaning and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the dataset from a CSV file**\n",
    "Here we are loading the data from the data set that we are using for the Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**\n",
    "We drop columns `'Year_of_Release'`, `'Developer'`, `'Publisher'`, and `'Platform'` from the dataset as they are not essential for our analysis. This also allows us to reduce the time required to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Dropping certain less important attributes and check for the missing values(null values)\n",
    "dataset.drop(columns = ['Year_of_Release', 'Developer', 'Publisher', 'Platform'], inplace = True)\n",
    "print(dataset.isna().sum())\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = dataset.groupby(\"Genre\")[\"Global_Sales\"].count().head(10)\n",
    "game = dataset.groupby(\"Genre\")[\"Global_Sales\"].count()\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Genre Analysis**\n",
    "We group the `dataset` by the `Genre` column and count the number of games in each genre, storing this information in the `game` variable.\n",
    "We create a bar plot to `visualize` the distribution of `game` `genres` and their `global sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data for the plot\n",
    "game_types = game.index\n",
    "global_sales = game.values\n",
    "# Create a modern color palette\n",
    "colors = ['#FF6F61', '#6B5B95', '#88B04B', '#F7DC6F', '#00A6A6', '#8D5A97', '#5B9AA0']\n",
    "# Create a bar plot with custom colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(game_types, global_sales, color=colors) # type: ignore\n",
    "plt.xlabel(\"Game Type\", fontsize=12)\n",
    "plt.ylabel(\"Global Sales (in millions)\", fontsize=12)\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.title(\"Global Sales by Game Type\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add grid lines for a modern look\n",
    "plt.show()\n",
    "# plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation Analysis**\n",
    "Now let’s have a look at the `correlation` between the features of this `dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualization based on correlation of the data\n",
    "print(dataset)\n",
    "non_numeric_columns = dataset.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "data_numeric = dataset.drop(columns=non_numeric_columns)\n",
    "corr_matrix = data_numeric.corr()\n",
    "# sns.set_theme(style=\"white\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"rainbow\", linewidths=0.5) # type: ignore\n",
    "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Video Game Sales Prediction Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataset into Train & Test sets**\n",
    "- Here now we initialize `'x'`, `'y'`.\n",
    "- <b>`'x'`</b> is the set of independant variables `NA_SALES`, `EU_SALES`, etc. and <b>`'y'`</b> is the target variable `GLOBAL_SALES`\n",
    "- We split the data into training and testing sets using the `train_test_split` function from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "x = dataset.iloc[:, :].values\n",
    "x = np.delete(x, 6, 1)\n",
    "y = dataset.iloc[:, 6:7].values\n",
    "# Splitting the dataset into Train and Test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the `names` of the games in both `train` and `test` and remove them before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving name of the games in training and test set\n",
    "games_in_training_set = x_train[:, 0]\n",
    "games_in_test_set = x_test[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `Name` column for training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the column that contains the name of the games\n",
    "x_train = x_train[:, 1:]\n",
    "x_test = x_test[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imputation**\n",
    "We use the `SimpleImputer` from `scikit-learn` to fill `missing numerical values` in columns 5, 6, 7, and 8 with their `mean` values.\n",
    "\n",
    "We fill `missing categorical values` in columns 0 and 9 with `'NA'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy = 'mean')\n",
    "x_train[:, [5 ,6, 7, 8]] = imputer.fit_transform(x_train[:, [5, 6, 7, 8]])\n",
    "x_test[:, [5 ,6, 7, 8]] = imputer.transform(x_test[:, [5, 6, 7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "categorical_imputer = SimpleImputer(strategy = 'constant', fill_value = 'NA')\n",
    "x_train[:, [0, 9]] = categorical_imputer.fit_transform(x_train[:, [0, 9]])\n",
    "x_test[:, [0, 9]] = categorical_imputer.transform(x_test[:, [0, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-Hot Encoder**\n",
    "We perform **`one-hot encoding`** using the `ColumnTransformer` and `OneHotEncoder` from the `scikit-learn` library to convert categorical variables into a format suitable for `machine learning`.This will assign one separate column to each category present in a categorical column of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0, 9])], remainder = 'passthrough')\n",
    "x_train = ct.fit_transform(x_train)\n",
    "x_test = ct.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building The Regression Model**\n",
    "We’ll implement our model i.e. the regressor using `XGBRegressor` (where XGB stands for extreme gradient boosting). `XGBoost` is an ensemble machine learning algorithm based on decision trees similar to the `RandomForest` algorithm. However, unlike `RandomForest` that makes use of fully grown trees, `XGBoost` combines trees that are not too deep. Also, the number of trees combined in `XGBoost` is more in comparison to `RandomForest`. Ensemble algorithms effectively combine weak learners to produce a strong learner. `XGBoost` has additional features focused on `performance` and `speed` when compared to gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(n_estimators = 200, learning_rate= 0.08)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Making predictions on the Test set**\n",
    "We created a DataFrame `'predictions'` to compare the `predicted global sales` with the `actual global sales`.<br>\n",
    "We generate a `plot` to visualize the predicted and actual sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "# Visualising actual and predicted sales\n",
    "games_in_test_set = games_in_test_set.reshape(-1, 1)\n",
    "y_pred = y_pred.reshape(-1, 1)\n",
    "predictions = np.concatenate([games_in_test_set, y_pred, y_test], axis = 1)\n",
    "predictions = pd.DataFrame(predictions, columns = ['Name', 'Predicted_Global_Sales', 'Actual_Global_Sales'])\n",
    "print(predictions[[\"Predicted_Global_Sales\", \"Actual_Global_Sales\"]])\n",
    "predictions.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing The Predicted Data with The Actual Global Sales**\n",
    "### **A Residual Plot**\n",
    "Here, we are creating a `residual plot` to visualize the differences(residuals) between the `predicted sales` and the `actual sales`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(y_pred, residuals, alpha=0.5, color='cyan')\n",
    "plt.xlabel(\"Predicted Sales\", fontsize=12, color='white')\n",
    "plt.ylabel(\"Residuals\", fontsize=12, color='white')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Residual Plot\", fontsize=14, color='white')\n",
    "plt.legend(handles=[scatter], labels=[\"Data Points\"], loc='upper right', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A Learning Curve**\n",
    "Here, we are creating `learning curves` to evaluate the performance of our `machine learning model`.\n",
    "Learning curves provide insights into how a model's performance changes as the size of the training dataset increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, x_train, y_train, cv=5, scoring='neg_mean_squared_error') # type: ignore\n",
    "train_rmse = np.sqrt(-train_scores)\n",
    "test_rmse = np.sqrt(-test_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_rmse.mean(axis=1), label='Training RMSE')\n",
    "plt.plot(train_sizes, test_rmse.mean(axis=1), label='Validation RMSE')\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Histogram Representation of The Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate subplots for actual and predicted sales histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax1.hist(y_test, bins=30, alpha=0.6, color='blue', label='Actual Sales')\n",
    "ax1.set_xlabel('Sales', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.set_title('Distribution of Actual Sales', fontsize=14)\n",
    "\n",
    "ax2.hist(y_pred, bins=30, alpha=0.6, color='green', label='Predicted Sales')\n",
    "ax2.set_xlabel('Sales', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.set_title('Distribution of Predicted Sales', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating The Model**\n",
    "The performance of the model is evaluated using the Performance Measures : `R Squared Score`, `Root Mean Square`, `Mean Absolute Error`\n",
    "\n",
    "-> Performance Measure of `R Squared Score` : The `more` the magnitude of `R2_Score` is closer to 1 the more accurate the model is<br>\n",
    "-> Performance measure of `Root Mean Square Error` : `Lower` the magnitude of `RMSE` more accurate the model is<br>\n",
    "-> Performance measure of `Mean Absolute Error` : `Lower` the magnitude of `MAE` more accurate the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "r2_score = r2_score(y_test, y_pred)\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"R Squared Score(R2_Score) of the model : {r2_score}\")\n",
    "print(f\"Root Mean Squared Error(RMSE) of the model : {rmse}\")\n",
    "print(f\"The Mean Absolute Error (MAE) of the model : {MAE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Observation**\n",
    "The predictive model developed in this project to estimate global sales of video games has yielded outstanding results. With an `R-squared score` of `0.975`, a `Root Mean Squared Error` (RMSE) of `0.305`, and a `Mean Absolute Error` (MAE) of `0.049`, the model demonstrates its exceptional accuracy and reliability.\n",
    "\n",
    "#### **R-Squared Score**\n",
    "The high `R-squared score` of `0.975` indicates that approximately `97.5%` of the variance in global sales can be explained by the model. This suggests that the chosen features and the `XGBoost regression model` are highly effective in capturing the underlying patterns in video game sales data.\n",
    "\n",
    "#### **Root Mean Squared Error (RMSE)**\n",
    "The `RMSE` value of `0.305` is notably `low`, signifying that the model's predictions are in close agreement with the actual sales figures. This level of accuracy is a testament to the model's ability to make precise estimations.\n",
    "\n",
    "#### **Mean Absolute Error (MAE)**\n",
    "The `MAE` value of `0.049` is exceptionally `small`, highlighting the model's ability to provide `highly accurate` estimates. A low MAE indicates that the model's predictions are, on average, only `0.049` units away from the actual sales figures, a testament to its reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "In summary, the developed `predictive model` serves as a valuable tool for the `video game industry`, providing a means to make `data-informed` decisions. `Game developers`, `publishers`, and `investors` can use this model to assess the potential success of their games with a high degree of confidence. The `remarkable accuracy` and `low errors` in the predictions showcase the model's `effectiveness` in capturing the dynamics of the `video game market`.\n",
    "\n",
    "This project's success sets the stage for `data-driven decision-making` in the gaming industry, enhancing the probability of creating and promoting successful `video games`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
